<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Agent</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.0.1/socket.io.js"></script>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
</head>
<body>
    <div class="main-container">
        <div class="sidebar">
            <button id="startButton" class="mic-button">Start Voice Agent</button>
            <div id="status" class="status">Status: Inactive</div>
            <div class="audio-controls">
                <div class="device-select">
                    <label for="voiceModel">Voice Model:</label>
                    <select id="voiceModel"></select>
                </div>
            </div>
            <div class="controls">
                <label class="toggle">
                    <input type="checkbox" id="showLogs">
                    <span class="slider"></span>
                </label>
                <label for="showLogs" class="toggle-label">Show Debug Logs</label>
            </div>
            <hr>
            <div class="logs-container syncscroll" name="log-container" id="logsContainer" style="display: none;">
                <div id="logs" class="logs"></div>
            </div>
        </div>
        <div class="content-container syncscroll" name="log-container">
            <div id="conversation" class="conversation-panel"></div>
        </div>
    </div>

    <!-- Press to Speak Button -->
    <div class="speak-button-container">
        <button id="speakButton" class="speak-button" disabled>
            <span class="speak-button-text">Hold to Speak</span>
        </button>
    </div>

    <script>
        document.addEventListener("DOMContentLoaded", () => {
            const startButton = document.getElementById('startButton');
            const statusDiv = document.getElementById('status');
            const logsDiv = document.getElementById('logs');
            const conversationDiv = document.getElementById('conversation');
            const voiceModelSelect = document.getElementById('voiceModel');
            const showLogsCheckbox = document.getElementById('showLogs');
            const logsContainer = document.getElementById('logsContainer');
            const speakButton = document.getElementById('speakButton');

            let socket = null;
            let audioContext = null;
            let audioWorkletNode = null;
            let microphoneStream = null;
            let isActive = false;
            let isMuted = true; // Start muted, only speak when button is pressed
            let isAgentSpeaking = false; // Track when agent is playing audio
            let audioQueue = [];
            let nextPlayTime = 0; // For continuous audio scheduling
            let lastAudioSendTime = 0;
            const AUDIO_SEND_INTERVAL = 20; // Send audio every 20ms (was 100ms - too aggressive)

            // --- UI and Logging ---

            function logMessage(message, type = 'info') {
                const logEntry = document.createElement('div');
                logEntry.className = `log-entry log-${type}`;
                logEntry.textContent = `[${new Date().toLocaleTimeString()}] ${message}`;
                logsDiv.appendChild(logEntry);
                logsDiv.scrollTop = logsDiv.scrollHeight;
            }

            function addConversationMessage(role, text) {
                const messageBubble = document.createElement('div');
                messageBubble.className = `message-bubble ${role}`;
                messageBubble.textContent = text;
                conversationDiv.appendChild(messageBubble);
                conversationDiv.scrollTop = conversationDiv.scrollHeight;
            }

            function setStatus(text) {
                statusDiv.textContent = `Status: ${text}`;
            }

            // --- Scroll Syncing ---
            const syncScrollContainers = document.querySelectorAll('.syncscroll');
            syncScrollContainers.forEach(el => {
                el.addEventListener('scroll', () => {
                    const scrollY = el.scrollTop;
                    syncScrollContainers.forEach(otherEl => {
                        if (otherEl !== el) {
                            otherEl.scrollTop = scrollY;
                        }
                    });
                });
            });

            // --- API Calls ---

            async function fetchVoiceModels() {
                try {
                    const response = await fetch('/tts-models');
                    const data = await response.json();
                    if (data.models) {
                        voiceModelSelect.innerHTML = '';
                        data.models.forEach(model => {
                            const option = document.createElement('option');
                            option.value = model.name;
                            option.textContent = model.display_name;
                            voiceModelSelect.appendChild(option);
                        });
                        voiceModelSelect.value = 'aura-2-thalia-en'; // Default
                    } else {
                        logMessage('Failed to load voice models.', 'error');
                    }
                } catch (error) {
                    logMessage(`Error fetching voice models: ${error}`, 'error');
                }
            }
            
            // --- Core Audio Logic ---

            async function startAudio() {
                if (isActive) return;
                isActive = true;
                setStatus('Initializing...');
                logMessage('Starting audio pipeline...');

                try {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
                    if (audioContext.state === 'suspended') await audioContext.resume();

                    await audioContext.audioWorklet.addModule('/audio-processor.js');
                    audioWorkletNode = new AudioWorkletNode(audioContext, 'audio-processor');

                    microphoneStream = await navigator.mediaDevices.getUserMedia({ audio: { sampleRate: 16000, channelCount: 1 }});
                    const microphone = audioContext.createMediaStreamSource(microphoneStream);
                    microphone.connect(audioWorkletNode);

                    audioWorkletNode.port.onmessage = (event) => {
                        if (socket && socket.connected && !isMuted && !isAgentSpeaking) {
                            // Send audio only when speak button is pressed and agent is not speaking
                            socket.emit('user_audio', event.data);
                            
                            // Log audio data being sent (occasionally)
                            if (Math.random() < 0.01) { // 1% of chunks
                                logMessage(`Sending user audio: ${event.data.byteLength} bytes`);
                            }
                        }
                    };
                    
                    logMessage('Audio pipeline ready.');
                    logMessage(`Microphone connected - Sample rate: ${audioContext.sampleRate}Hz`);
                    logMessage(`Audio mode: Press to Speak button only`);
                    connectSocket();

                } catch (error) {
                    logMessage(`Failed to start audio: ${error}`, 'error');
                    if (error.name === 'NotAllowedError') {
                        logMessage('Microphone permission denied. Please allow microphone access and try again.', 'error');
                    } else if (error.name === 'NotFoundError') {
                        logMessage('No microphone found. Please connect a microphone and try again.', 'error');
                    }
                    setStatus('Error');
                    isActive = false;
                }
            }

            function stopAudio() {
                if (!isActive) return;
                isActive = false;
                logMessage('Stopping audio pipeline...');
                if (socket) {
                    socket.emit('stop_voice_agent');
                    socket.disconnect();
                }
                if (microphoneStream) {
                    microphoneStream.getTracks().forEach(track => track.stop());
                }
                if (audioContext && audioContext.state !== 'closed') {
                    audioContext.close();
                }
                audioQueue = [];
                isPlaying = false;
                setStatus('Inactive');
                startButton.textContent = 'Start Voice Agent';
            }

            function playNextAudioChunk() {
                if (audioQueue.length === 0 || !audioContext) {
                    // Re-enable speak button when agent finishes speaking
                    if (isAgentSpeaking) {
                        setTimeout(() => {
                            if (audioQueue.length === 0) { // Double check after delay
                                isAgentSpeaking = false;
                                speakButton.disabled = false;
                                speakButton.style.opacity = '1';
                                logMessage('Agent finished speaking - Hold to Speak enabled');
                            }
                        }, 500); // Small delay to ensure all audio is finished
                    }
                    return;
                }

                const audioData = audioQueue.shift();
                
                if (audioContext.state === 'suspended') {
                    audioContext.resume().then(() => {
                        playRawPCM(audioData);
                    });
                } else {
                    playRawPCM(audioData);
                }
            }

            function playRawPCM(audioData) {
                try {
                    // Convert Uint8Array to Int16Array (16-bit PCM)
                    const numSamples = audioData.length / 2;
                    const pcmData = new Int16Array(audioData.buffer, audioData.byteOffset, numSamples);
                    
                    // Create AudioBuffer
                    const audioBuffer = audioContext.createBuffer(1, numSamples, 24000);
                    const channelData = audioBuffer.getChannelData(0);
                    
                    // Convert Int16 to Float32 and fill the buffer
                    for (let i = 0; i < numSamples; i++) {
                        const sample = pcmData[i];
                        channelData[i] = sample / 32768.0;
                    }
                    
                    // Schedule the buffer for continuous playback
                    const source = audioContext.createBufferSource();
                    source.buffer = audioBuffer;
                    source.connect(audioContext.destination);
                    
                    // Calculate when to start this chunk for smooth playback
                    const currentTime = audioContext.currentTime;
                    if (nextPlayTime <= currentTime) {
                        nextPlayTime = currentTime + 0.01; // Small buffer to avoid glitches
                    }
                    
                    source.start(nextPlayTime);
                    nextPlayTime += audioBuffer.duration; // Schedule next chunk right after this one
                    
                    // Log less frequently
                    if (Math.random() < 0.05) { // 5% of chunks (was 10%)
                        logMessage(`Playing audio chunk: ${numSamples} samples, scheduled at ${nextPlayTime.toFixed(3)}s`);
                    }
                    
                } catch (error) {
                    logMessage(`Error playing raw PCM: ${error}`, 'error');
                    console.error('Raw PCM playback error:', error);
                }
            }

            // --- WebSocket Logic ---

            function connectSocket() {
                if (socket && socket.connected) return;

                socket = io();

                socket.on('connect', () => {
                    logMessage('Socket connected successfully.');
                    setStatus('Connected, starting agent...');
                    const sessionConfig = {
                        voiceModel: voiceModelSelect.value,
                    };
                    socket.emit('start_voice_agent', sessionConfig);
                });

                socket.on('disconnect', () => {
                    logMessage('Socket disconnected.', 'warn');
                    stopAudio(); // Full cleanup on disconnect
                });

                socket.on('agent_response', (data) => {
                    logMessage(`Agent Response: ${JSON.stringify(data)}`);
                    switch (data.type) {
                        case 'Welcome':
                            setStatus('Agent Ready');
                            // Show current audio mode status
                            logMessage('Agent ready - Use Press to Speak button');
                            setStatus('Agent Ready - Press button to speak');
                            // Enable the speak button when agent is ready
                            speakButton.disabled = false;
                            break;
                        case 'ConversationText':
                            addConversationMessage('assistant', data.content);
                            break;
                        case 'Error':
                            logMessage(`Agent Error: ${data.description}`, 'error');
                            setStatus('Error');
                            break;
                    }
                });

                socket.on('agent_audio', (chunk) => {
                    audioQueue.push(new Uint8Array(chunk));
                    // Disable speak button when agent starts speaking
                    if (!isAgentSpeaking) {
                        isAgentSpeaking = true;
                        speakButton.disabled = true;
                        speakButton.style.opacity = '0.6';
                        logMessage('Agent speaking - Hold to Speak disabled');
                        // Stop user speaking if they were speaking
                        stopSpeaking();
                    }
                    playNextAudioChunk();
                });
            }

            // --- Event Listeners ---

            startButton.addEventListener('click', () => {
                if (isActive) {
                    stopAudio();
                } else {
                    startAudio();
                    startButton.textContent = 'Stop Voice Agent';
                }
            });

            showLogsCheckbox.addEventListener('change', (e) => {
                if (e.target.checked) {
                    logsContainer.style.display = 'block';
                    logMessage('Debug logs are now visible');
                } else {
                    logsContainer.style.display = 'none';
                }
            });

            // Press-and-hold functionality for speak button
            function startSpeaking() {
                if (!isAgentSpeaking && isActive) {
                    isMuted = false;
                    logMessage('Speaking...', 'user');
                    setStatus('Listening...');
                    speakButton.querySelector('.speak-button-text').textContent = 'Speaking...';
                    speakButton.style.background = 'linear-gradient(135deg, #28a745, #1e7e34)';
                }
            }

            function stopSpeaking() {
                isMuted = true;
                logMessage('Stopped speaking.');
                setStatus('Agent Ready - Press button to speak');
                speakButton.querySelector('.speak-button-text').textContent = 'Hold to Speak';
                speakButton.style.background = 'linear-gradient(135deg, #007bff, #0056b3)';
            }

            // Mouse events
            speakButton.addEventListener('mousedown', (e) => {
                e.preventDefault();
                startSpeaking();
            });

            speakButton.addEventListener('mouseup', () => {
                stopSpeaking();
            });

            speakButton.addEventListener('mouseleave', () => {
                stopSpeaking();
            });

            // Touch events for mobile
            speakButton.addEventListener('touchstart', (e) => {
                e.preventDefault();
                startSpeaking();
            });

            speakButton.addEventListener('touchend', (e) => {
                e.preventDefault();
                stopSpeaking();
            });

            fetchVoiceModels();
        });
    </script>
</body>
</html>